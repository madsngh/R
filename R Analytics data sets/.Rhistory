some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
tweet_clean = clean.text(tweet_txt)
mach_corpus = Corpus(VectorSource(tweet_clean))
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(mach_corpus,
control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
wordcloud(dm$word, dm$freq, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
tweets <- searchTwitter("hadoop", n=200) # top 25 tweets that contain search term
tweet_txt = sapply(tweets, function(x) x$getText())
tweetsDF <- twListToDF(tweets)
head(tweetsDF)
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
tweet_clean = clean.text(tweet_txt)
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
# create a corpus
mach_corpus = Corpus(VectorSource(tweet_clean))
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(mach_corpus,
control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
# save the image in png format
png("rstat.png", width=12,
height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
example("inherits")
mach_corpus
tweets <- searchTwitter("Dexlab", n=200) # top 25 tweets that contain search term
tweet_txt = sapply(tweets, function(x) x$getText())
tweetsDF <- twListToDF(tweets)
head(tweetsDF)
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
tweet_clean = clean.text(tweet_txt)
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
# create a corpus
mach_corpus = Corpus(VectorSource(tweet_clean))
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(mach_corpus,
control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
# save the image in png format
png("rstat.png", width=12,
height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
tweets <- searchTwitter("Dexlab Analytics", n=200) # top 25 tweets that contain search term
tweet_txt = sapply(tweets, function(x) x$getText())
tweetsDF <- twListToDF(tweets)
head(tweetsDF)
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
names(some_txt) = NULL
return(some_txt)
}
tweet_clean = clean.text(tweet_txt)
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
# create a corpus
mach_corpus = Corpus(VectorSource(tweet_clean))
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(mach_corpus,
control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
# save the image in png format
png("rstat.png", width=12,
height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
tweets <- searchTwitter("Simran Kaur", n=200) # top 25 tweets that contain search term
tweet_txt = sapply(tweets, function(x) x$getText())
tweetsDF <- twListToDF(tweets)
head(tweetsDF)
clean.text <- function(some_txt)
{
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
some_txt = gsub("@\\w+", "", some_txt)
some_txt = gsub("[[:punct:]]", "", some_txt)
some_txt = gsub("[[:digit:]]", "", some_txt)
some_txt = gsub("http\\w+", "", some_txt)
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
some_txt = gsub("amp", "", some_txt)
# define "tolower error handling" function
try.tolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
return(y)
}
some_txt = sapply(some_txt, try.tolower)
some_txt = some_txt[some_txt != ""]
return(some_txt)
}
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
# create a corpus
mach_corpus = Corpus(VectorSource(tweet_clean))
# create document term matrix applying some transformations
y = tolower(x)
tweet_clean = clean.text(tweet_txt)
names(some_txt) = NULL
tdm = TermDocumentMatrix(mach_corpus,
control = list(removePunctuation = TRUE,
stopwords = c("machine", "learning", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
# save the image in png format
png("rstat.png", width=12,
height=8, units="in", res=300)
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
library(Hmisc)
abc <- spss.get(file.choose())
head(abc)
install.packages("sem")
install.packages("lavaan")
examData <- read.delim ( file.choose (), header=TRUE)
examData <- read.csv ( "Exam.csv", header=TRUE)
setwd("D:/R datasets")
examData <- read.csv ( "Exam.csv", header=TRUE)
head(examData)
examNumeric<-examData[, c(2:4)]
cor(examNumeric, use="everything", method="pearson")
cor.test (examData$Exam,examData$Anxiety , method= "pearson")
ExamMatrix<- as.matrix (examData[, c("Exam", "Anxiety", "Revise")])
Hmisc::rcorr (ExamMatrix, type= "pearson")
examData2 <- examData [, c ("Exam", "Anxiety", "Revise")]
cor (examData2)^2*100
liarData <- read.csv ("Liar.csv", header=TRUE)
head(liarData)
cor (liarData$Position, liarData$Creativity, use= "complete.obs", method="spearman")
liarMatrix <- as.matrix (liarData [, c (1:2)])
Hmisc:: rcorr (liarMatrix, type="spearman")
cor (liarData$Position, liarData$Creativity,method="kendall")
library (ggm)
Partial<- pcor (c("Exam", "Anxiety","Revise"), var (examData))
Partial
Partial^2*100
partial2<- pcor (c("Revise","Exam","Anxiety"), var(examData))
partial2^2*100
examData1 <- na.omit(examData)
Partial<- pcor (c("Exam", "Anxiety","Revise"), var (examData1))
Partial
Partial^2*100
Partial<- pcor (c("Exam", "Revise","Anxiety"), var (examData1))
Partial
Partial^2*100
Partial<- pcor (c("Anxiety", "Revise","Exam"), var (examData1))
Partial
Partial^2*100
pcor.test (Partial,1,103)
setwd("D:/R analytics")
Album1 <- read.delim ("Album Sales.dat", header=TRUE)
head(Album1)
AlbumSales <- lm(sales ~ adverts, data=Album1, na.action=na.omit)
summary (AlbumSales)
cor (Album1 [, c("sales", "adverts")])
(.578)^2
food <- c(10, 28)
affection <- c(114, 48)
catsTable <- cbind(food, affection)
library(gmodels)
CrossTable(catsTable, fisher = TRUE, chisq = TRUE,
expected = TRUE, sresid = TRUE,format = "SPSS")
log(.159)
exp(.159)
CrossTable(catsTable, fisher = TRUE, chisq = TRUE,
expected = TRUE, sresid = TRUE,format = "SAS")
cats <- read.csv(file.choose())
head(cats)
CrossTable(cats$Training,cats$Dance, fisher = TRUE, chisq = TRUE,
expected = TRUE, sresid = TRUE,format = "SAS")
mydata <- read.sas7bdat(file.choose())
library(sas7bdat)
mydata <- read.sas7bdat(file.choose())
View(mydata)
colnames(mydata) <- tolower(colnames(mydata))
xtabs(~admit + rank, data = mydata)
CrossTable(mydata$admit, mydata$rank)
CrossTable(mydata$admit, mydata$rank prop.r=F, prop.c=F)
CrossTable(mydata$admit, mydata$rank ,prop.r=F, prop.c=F)
CrossTable(mydata$admit, mydata$rank ,prop.r=F, prop.c=F,prop.t=F)
a <- (33/61)/(28/61)
a
b <- (12/67)/(55/67)
b
a/b
mydata$admit <- factor(mydata$admit)
mydata$rank<- factor(mydata$rank)
intcpt <- glm (admit~1, data=mydata, family = "binomial")
summary(intcpt)
mydata <- within(mydata, rank <- relevel(rank, ref = 4))
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
exp(coef(mylogit))
b/a
a <- c1,2,3)
a <- c(1,2,3)
b <- c(1,2)
a*b
attach(women)
View(women)
fn<-lm(formula = weight~height, data=women)
summary(fn)
plot(fn)
plot(fn)
View(women)
predict.lm(fn)
pizza<-read.csv(file.choose())
View(pizza)
t.test(pizza, alternative="greater", mu=0) # given h0: mu=0 ; h1: mu>0
Album2 <- read.delim ("Album Sales2.dat", header=TRUE)
head(Album2)
AlbumSReg <- lm (sales~adverts, data=Album2, na.action=na.omit)
summary(AlbumSReg)
AlbumMReg<- lm (sales~ adverts+ airplay+ attract, data=Album2, na.action=na.omit)
summary (AlbumMReg)
library(pastecs)
fit<- lm (scale(sales)~ scale(adverts)+ scale (airplay)+ scale (attract), data=Album2, na.action=na.omit)
summary (fit)
library (QuantPsyc)
library (psych)
library (pastecs)
lm.beta (AlbumMReg)
round (stat.desc (Album2, basic=FALSE, norm=TRUE), digit=3)
confint (AlbumMReg)
anova (AlbumSReg, AlbumMReg)
confint (AlbumMReg)
AIC (AlbumSReg)
AIC (AlbumMReg)
library(sas7bdat)
walmart <- read.sas7bdat(file.choose())
View(walmart)
ranuni <- sample(x=c("Training","Testing"),
size=nrow(walmart),replace=T,
prob=c(0.7,0.3))
set.seed(25) # setting the random number seed for splitting the dataset
ranuni <- sample(x=c("Training","Testing"),
size=nrow(walmart),replace=T,
prob=c(0.7,0.3))
ranuni
cbind(ranuni)
TrainingData <- walmart[ranuni=="Training",] # generating the training data
View(TrainingData)
TestingData <- walmart[ranuni=="Testing",] # generating the testing data
TrainingModel <- lm(Employee_Performance~., data= TrainingData)
summary(TrainingModel1)
TrainingModel <- lm(Employee_Performance~., data= TrainingData)
summary(TrainingModel)
library(car)
vif(TrainingModel)  #to check for the Multicollinearity   #VIFs less than 5 so no problem
TrainingModel1 <- lm(Employee_Performance~., data= TrainingData[, -14])
summary(TrainingModel)
summary(TrainingModel1)
durbinWatsonTest(TrainingModel)     #Null Hyp: rho = 0
durbinWatsonTest(TrainingModel1)     #Null Hyp: rho = 0
qqnorm(TrainingModel$residuals)  #qq plot
shapiro.test(TrainingModel1$residuals)    #checking normALITY OF RESIDUALS. SAMPLE SIZE IS LESS. Works good when n > 2000.
TrainingModel2 <- step(object= TrainingModel, direction="backward")
summary(TrainingModel2)    #ALL VAR(s) mentioned ARE IMPACTING THE Y VARIABLE
TestPred <- predict.lm(object= TrainingModel2, newdata = TestingData)
TestPred    #estimated values of y
cor(x= TestPred, y = TestingData$Employee_Performance)  #corr high means good
plot(x= TestingData$Employee_Performance,y= TestPred, type= "p", col= "red")   #plot looks linear then good
cats <- read.csv(file.choose())
head(cats)
library(gmodels)
CrossTable(cats$Training,cats$Dance, fisher = TRUE, chisq = TRUE,
expected = TRUE, sresid = TRUE,format = "SPSS")
mydata <- read.sas7bdat(file.choose())
View(mydata)
colnames(mydata) <- tolower(colnames(mydata))
xtabs(~admit, data=mydata)
xtabs(~admit + rank, data = mydata)
str(mydata)
mydata$admit <- factor(mydata$admit)
mydata$rank<- factor(mydata$rank)
intcpt <- glm (admit~1, data=mydata, family = "binomial")
summary(intcpt)
xtabs(~admit, data=mydata)
a <- 127/400
b <- a/(1-a)
b
log(b)
exp(b)/(1+exp(b))
c <- log(b)
exp(c)/(1+exp(c))
View(mydata)
mydata <- within(mydata, rank <- relevel(rank, ref = 4))
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
exp(coef(mylogit))
mydata <- within(mydata, rank <- relevel(rank, ref = 1))
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
exp(coef(mylogit))
mydata <- read.sas7bdat(file.choose())
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
colnames(mydata) <- tolower(colnames(mydata))
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
mydata$admit <- factor(mydata$admit)
mydata$rank<- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
exp(coef(mylogit))
modelChi <- mylogit$null.deviance - mylogit$deviance
chidf <- mylogit$df.null - mylogit$df.residual
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
modelChi
chidf
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob
GermanBank <- read.csv("german_bank.csv")
getwd()
GermanBank <- read.csv("german_bank.csv")
setwd("D:/R/R Analytics data sets")
GermanBank <- read.csv("german_bank.csv")
View(GermanBank)
str(GermanBank)
german_bank <- GermanBank[,2:32]
for(i in c(1,3:9,11:21,23:31)){
german_bank[,i] <- factor(x = german_bank[,i], levels= sort(unique(german_bank[,i])))
}
str(german_bank)
class0 <- german_bank[german_bank$RESPONSE == 0, ]
nrow(class0)
class1 <-german_bank[german_bank$RESPONSE ==1,]
nrow(class1)
ranuni <- sample(x=c("Training","Testing"),size= nrow(german_bank),
replace= T, prob= c(0.7,0.3))
Trngdt <- german_bank[ranuni == "Training", ]
Trclass0 <- Trngdt[Trngdt$RESPONSE == 0, ]
Trclass1 <- Trngdt[Trngdt$RESPONSE == 1, ]
nrow(Trclass0)
nrow(Trclass1)
Tstdt <- german_bank[ranuni == "Testing",]
Training_model <- glm(RESPONSE~. , data= Trngdt, family= "binomial")
summary(Training_model)
library(MASS)
Training_model <- stepAIC(object = Training_model, direction = "backward")
Training_model2 <- glm(RESPONSE ~ CHK_ACCT + DURATION + HISTORY + NEW_CAR + USED_CAR +
EDUCATION + AMOUNT + SAV_ACCT + INSTALL_RATE + MALE_SINGLE +
CO_APPLICANT + GUARANTOR + REAL_ESTATE + PROP_UNKN_NONE +
OTHER_INSTALL + RENT + TELEPHONE + FOREIGN, data=Trngdt,
family = "binomial")
summary(Training_model2)
library(pROC)
troc <- roc(response =Training_model$y,
predictor = Training_model$fitted.values,plot = T )
troc$auc
Training_model$fitted.values
Trclass0
nrow(Trclass0)
nrow(Trclass1)
table(trPred)
trPred <- ifelse(test = Training_model$fitted.values < 0.5, yes = 0, no = 1) #cut-off is 0.5
table(trPred)
table(Training_model$y,trPred)
(116+449)/(116+449+48+93)
tsPred <- predict.glm(object = Training_model, newdata = Tstdt, type = "response")
tsPred
tsroc <- roc(response= Tstdt$RESPONSE, predictor = tsPred, plot= T)
tsroc$auc
tsPred <- ifelse(test= tsPred <0.5, yes= 0, no = 1)
table(Tstdt$RESPONSE,tsPred)
(34+179)/(34+179+24+55)
